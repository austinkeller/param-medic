#!/usr/bin/env python
"""
Analyze pairs of closely-eluting spectra with similar precursor and fragments,
infer precursor and fragment error, and transform those values into values usable
as tolerance parameters by search engines.

If multiple files are specified, they will be processed together.
"""

import argparse
import logging
from datetime import datetime
from parammedic import errorcalc, ms2_io, mzml_io, mod_inference, enzyme_inference, util, binning
from parammedic import __version__
import gzip
from collections import OrderedDict

__author__ = "Damon May"
__copyright__ = "Copyright (c) 2016 Damon May"
__license__ = "Apache 2.0"

logger = logging.getLogger(__name__)
SPECTRUM_PROCESSED_REPORT_INTERVAL = 1000

# if we see fewer spectra than this, everything is questionable
MIN_SPECTRA_FOR_CONFIDENCE = 2000


def declare_gather_args():
    """
    Declare all arguments, parse them, and return the args dict.
    Does no validation beyond the implicit validation done by argparse.
    return: a dict mapping arg names to values
    """

    # declare args
    parser = argparse.ArgumentParser(description=__doc__)
    parser.add_argument('infiles', type=argparse.FileType('r'), nargs='+',
                        help='input .mzML or .ms2 file(s). If multiple files are specified, \
                        they may be processed separately or together')

    parser.add_argument('--min-precursor-mz', type=float_greaterthanequalto0_type,
                        default=errorcalc.DEFAULT_MIN_MZ_FOR_BIN_PRECURSOR,
                        help='minimum precursor m/z value to use')
    parser.add_argument('--max-precursor-mz', type=float_greaterthanequalto0_type,
                        default=errorcalc.DEFAULT_MAX_MZ_FOR_BIN_PRECURSOR,
                        help='maximum precursor m/z value to use')
    parser.add_argument('--min-frag-mz', type=float_greaterthanequalto0_type,
                        default=errorcalc.DEFAULT_MIN_MZ_FOR_BIN_FRAGMENT,
                        help='minimum fragment m/z value to use')
    parser.add_argument('--max-frag-mz', type=float_greaterthanequalto0_type,
                        default=errorcalc.DEFAULT_MAX_MZ_FOR_BIN_FRAGMENT,
                        help='maximum fragment m/z value to use')
    parser.add_argument('--max-precursor-delta-ppm', type=float_greaterthanequalto0_type,
                        default=errorcalc.DEFAULT_MAX_PRECURSORDIST_PPM,
                        help='maximum ppm distance between precursor m/z values to consider two scans potentially \
                        generated by the same peptide')
    parser.add_argument('--charge', type=int_greaterthanequalto1_type,
                        default=errorcalc.DEFAULT_CHARGE,
                        help='charge state to consider MS/MS spectra from')
    parser.add_argument('--min-scan-frag-peaks', type=int_greaterthanequalto1_type,
                        default=util.MIN_SCAN_PEAKS,
                        help='Minimum fragment peaks an MS/MS scan must contain to be considered')
    parser.add_argument('--top-n-frag-peaks', type=int_greaterthanequalto1_type,
                        default=errorcalc.DEFAULT_TOPN_FRAGPEAKS,
                        help='number of most-intense fragment peaks to consider, per MS/MS spectrum')
    parser.add_argument('--min-common-frag-peaks', type=int_greaterthanequalto1_type,
                        default=errorcalc.DEFAULT_MIN_FRAGPEAKS_INCOMMON,
                        help='number of the most-intense peaks that two spectra must share in order to be \
                             potentially generated by the same peptide')
    parser.add_argument('--pair-top-n-frag-peaks', type=int_greaterthanequalto1_type,
                        default=errorcalc.DEFAULT_TOPN_FRAGPEAKS_FOR_ERROR_EST,
                        help='number of fragment peaks per spectrum pair to be used in fragment error estimation')
    parser.add_argument('--max-scan-separation', type=int_greaterthanequalto1_type,
                        default=errorcalc.DEFAULT_MAX_SCANS_BETWEEN_COMPARESCANS,
                        help='maximum number of scans two spectra can be separated by in order to be considered \
                        potentially generated by the same peptide')
    parser.add_argument('--min-peak-pairs', type=int_greaterthanequalto1_type,
                        default=errorcalc.DEFAULT_MIN_PEAKPAIRS_FOR_DISTRIBUTION_FIT,
                        help='minimum number of peak pairs (for precursor or fragment) that must be successfully \
                        paired in order to attempt to estimate error distribution')
    parser.add_argument('--process-together', action="store_true",
                        help='process multiple files as though they were a single file (default: process \
                         multiple files separately)')
    parser.add_argument('--noerrorcalc', action="store_true",
                        help='suppress precursor and fragment error calculation')
    parser.add_argument('--nomodinference', action="store_true",
                        help='suppress modification inference')
    parser.add_argument('--enzyme', action="store_true",
                        help='infer digestion enzyme')

    parser.add_argument('--debug', action="store_true", help='Enable debug logging')
    parser.add_argument('--version', action='version', version='%(prog)s {version}'.format(version=__version__))

    # these parameters are for development
    parser.add_argument('--maxspectra', type=int, help='Maximum spectra to process per file (or total, if process-together)')

    return parser.parse_args()


def int_greaterthanequalto1_type(x):
    """
    parameter type that is an integer >= 1
    :param x:
    :return:
    """
    x = int(x)
    if x < 1:
        raise argparse.ArgumentTypeError("Minimum value is 1")
    return x


def float_greaterthanequalto0_type(x):
    """
    parameter type that is a float >= 0
    :param x:
    :return:
    """
    x = float(x)
    if x < 0:
        raise argparse.ArgumentTypeError("Minimum value is 0.0")
    return x


def main():
    """
    Handle arguments, create an ErrorCalculator, process all the spectra, and infer parameters.
    :return:
    """
    args = declare_gather_args()
    if args.max_precursor_mz <= args.min_precursor_mz:
        quit("max-precursor-mz must be > min-precursor-mz")
    if args.max_frag_mz <= args.min_frag_mz:
        quit("max-frag-mz must be > min-frag-mz")

    # logging
    logging.basicConfig(level=logging.INFO,
                        format="%(asctime)s %(levelname)s: %(message)s")
    if args.debug:
        logger.setLevel(logging.DEBUG)
        # any module-specific debugging goes below
        errorcalc.logger.setLevel(logging.DEBUG)
        mod_inference.logger.setLevel(logging.DEBUG)
        enzyme_inference.logger.setLevel(logging.DEBUG)
        ms2_io.logger.setLevel(logging.DEBUG)
        mzml_io.logger.setLevel(logging.DEBUG)
        util.logger.setLevel(logging.DEBUG)

    script_start_time = datetime.now()
    logger.debug("Start time: %s" % script_start_time)
    
    # each member of file_groups is a list of files that will be processed together
    infile_groups = []
    if args.process_together:
        # process all files as a single group
        infile_groups.append(args.infiles)
    else:
        # process each file separately
        for infile in args.infiles:
            infile_groups.append([infile])

    if args.process_together:
        print("Processing all input files together.")
    elif len(infile_groups) > 1:
        print("Processing input files separately.")

    output_name_value_dicts = []
    for infile_group in infile_groups:
        error_calculator = None
        run_attribute_detectors = []
        modification_detectors = []
        if not args.noerrorcalc:
            error_calculator = errorcalc.ErrorCalculator(min_precursor_mz=args.min_precursor_mz,
                                                         max_precursor_mz=args.max_precursor_mz,
                                                         min_frag_mz=args.min_frag_mz,
                                                         max_frag_mz=args.max_frag_mz,
                                                         charge=args.charge,
                                                         min_scan_frag_peaks=args.min_scan_frag_peaks,
                                                         topn_frag_peaks=args.top_n_frag_peaks,
                                                         min_common_frag_peaks=args.min_common_frag_peaks,
                                                         pair_topn_frag_peaks=args.pair_top_n_frag_peaks,
                                                         max_scan_separation=args.max_scan_separation,
                                                         max_precursor_deltappm=args.max_precursor_delta_ppm,
                                                         min_peakpairs=args.min_peak_pairs)
            run_attribute_detectors.append(error_calculator)
    
        if not args.nomodinference:
            silac_detector = mod_inference.SILACDetector()
            reporter_detector = mod_inference.ReporterIonProportionCalculator()
            phospho_detector = mod_inference.PhosphoLossProportionCalculator()
            modification_detectors.extend([silac_detector, reporter_detector, phospho_detector])
        run_attribute_detectors.extend(modification_detectors)
    
        n_spectra_processed = 0
        is_first_file = True
        for infile in infile_group:
            if not is_first_file:
                for run_attribute_detector in run_attribute_detectors:
                    run_attribute_detector.next_file()
            logger.info("Processing input file %s..." % infile.name)
            for spectrum in generate_spectra(infile, args.min_scan_frag_peaks):
                binned_spectrum = binning.bin_spectrum(spectrum.mz_array, spectrum.intensity_array)
                if n_spectra_processed % SPECTRUM_PROCESSED_REPORT_INTERVAL == 0:
                    logger.debug("  processed %d total spectra..." % n_spectra_processed)
                for run_attribute_detector in run_attribute_detectors:
                    run_attribute_detector.process_spectrum(spectrum, binned_spectrum)
                n_spectra_processed += 1
                if (args.maxspectra is not None) and n_spectra_processed >= args.maxspectra:
                    logger.info("Stopping early after %d spectra." % n_spectra_processed)
                    break
            infile.close()
            is_first_file = False
            if (args.maxspectra is not None) and n_spectra_processed >= args.maxspectra:
                break
        if n_spectra_processed == 0:
            print("No spectra found! Quitting.")
            quit(1)
    
        logger.debug("Processed all spectra. Summarizing...")
        output_name_value_dict = OrderedDict()
        output_name_value_dicts.append(output_name_value_dict)
        if len(infile_group) == 1:
            output_name_value_dict['file'] = infile_group[0].name
        else:
            output_name_value_dict['file'] = 'multiple'

        search_parameter_recommendations = []
        if error_calculator:
            errorcalc_messages, precursor_sigma_ppm, frag_sigma_ppm, precursor_prediction_ppm, fragment_prediction_th = error_calculator.summarize()
            search_parameter_recommendations.extend(errorcalc_messages)
            # assemble error-calculation output. If a prediction failed the value will be None, so reformat
            output_name_value_dict['precursor_prediction_ppm'] = str(precursor_prediction_ppm) if precursor_prediction_ppm is not None else 'ERROR'
            output_name_value_dict['precursor_sigma_ppm'] = str(precursor_sigma_ppm) if precursor_sigma_ppm is not None else 'ERROR'
            output_name_value_dict['fragment_prediction_th'] = str(fragment_prediction_th) if fragment_prediction_th is not None else 'ERROR'
            output_name_value_dict['fragment_sigma_ppm'] = str(frag_sigma_ppm) if frag_sigma_ppm is not None else 'ERROR'

        detected_modifications = []
        for modification_detector in modification_detectors:
            detector_result = modification_detector.summarize()
            detected_modifications.extend(detector_result.search_modifications)
            for name in detector_result.name_value_pairs:
                output_name_value_dict[name] = detector_result.name_value_pairs[name]

        search_parameter_recommendations.extend(detected_modifications)
    
        if args.enzyme:
            # go back through the input files
            print("Inferring enzyme...")
            n_spectra_processed = 0
            enzyme_detector = enzyme_inference.EnzymeDetector(detected_modifications)
            is_first_file = True
            for infile in infile_group:
                print("Processing input file %s..." % infile.name)
                reopened_infile = open(infile.name)
                if not is_first_file:
                    for run_attribute_detector in run_attribute_detectors:
                        run_attribute_detector.next_file()
                logger.debug("Processing input file %s..." % infile.name)
                for spectrum in generate_spectra(reopened_infile):
                    binned_spectrum = binning.bin_spectrum(spectrum.mz_array, spectrum.intensity_array)
                    if n_spectra_processed % SPECTRUM_PROCESSED_REPORT_INTERVAL == 0:
                        logger.debug("  processed %d total spectra..." % n_spectra_processed)
                    enzyme_detector.process_spectrum(spectrum, binned_spectrum)
                    n_spectra_processed += 1
                    if (args.maxspectra is not None) and n_spectra_processed >= args.maxspectra:
                        logger.info("Stopping early after %d spectra." % n_spectra_processed)
                        break
                reopened_infile.close()
                is_first_file = False
            print("Enzyme: %s" % enzyme_detector.summarize())
    
        if search_parameter_recommendations:
            print("\nSearch parameter recommendations:")
            for modification in search_parameter_recommendations:
                print(modification)
        else:
            print("No modifications detected requiring search parameter changes.")
        if n_spectra_processed < MIN_SPECTRA_FOR_CONFIDENCE:
            print("WARNING: only %d spectra were analyzed. Confidence in all assessments is low.")

    # print tab-separated output
    print("\t".join(output_name_value_dicts[0].keys()))
    for outdict in output_name_value_dicts:
        print("\t".join(trim_val(outdict[x]) for x in outdict))


def trim_val(x):
    """
    if x looks vaguely like a number, trim it to 5 decimal places. If not, leave it alone.
    This is brittle and silly, but it does the job for this specific use case.
    :param x:
    :return:
    """
    if "." in x:
        try:
            return "%.5f" % float(x)
        except ValueError:
            pass
    return x


def generate_spectra(spectra_file, min_scan_frag_peaks):
    """
    a generator for spectra from a .ms2 or .mzML file
    :param spectra_file:
    :return:
    """
    handle = spectra_file
    if spectra_file.name.endswith('.gz'):
        handle = gzip.open(spectra_file.name)
    if '.ms2' in spectra_file.name:
        io_module = ms2_io
    elif '.mzML' in spectra_file.name:
        io_module = mzml_io
    else:
        raise ValueError('generate_spectra, can\'t determine file type from name. Name=%s' % spectra_file.name)

    for spectrum in io_module.read_ms2_scans(handle):
        # check peak count against threshold
        if len(spectrum.mz_array) >= min_scan_frag_peaks:
            yield spectrum

start_time = datetime.now()
logger.debug("Start time: %s" % start_time)
main()
logger.debug("End time: %s" % datetime.now())
logger.debug("Elapsed time: %s" % (datetime.now() - start_time))
